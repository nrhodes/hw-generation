# -*- coding: utf-8 -*-
"""Char generation PyTorch.ipynb
"""
#%%
3+4
#%%

import matplotlib.pyplot as pyplot
from pathlib import Path
import random
from pytorch_lightning.callbacks import Callback



hwdir = Path('/data') / 'neil' / 'hw'

args = {
    # for model
    'epochs':57,
    'batch_size': 512,
    'lr': .001,
    'optimizer': 'adam',
    "rnn_hidden_size": 200,
    "tbtt_length": 60,
    "rnn_type": "gru",
    'num_components': 1,

    # for generation
    "generated_length": 100,
}



#%%
"""# Initialization"""


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
import pytorch_lightning as pl

#torch.manual_seed(args["seed"])
use_cuda = torch.cuda.is_available()
device = torch.device("cuda")


import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import Sampler

def plot_stroke(stroke, prompt=None, remainder=None, save_name=None):
    # Plot a single example.
    #print(f'stroke before penup: {stroke}')
    f, ax = pyplot.subplots()

    if torch.is_tensor(stroke):
        stroke = stroke.numpy()
    if torch.is_tensor(prompt):
        prompt = prompt.numpy()
    if torch.is_tensor(remainder):
        remainder = remainder.numpy()

    first_x = 0
    first_y = 0
    if prompt is not None:
        x_prompt = np.cumsum(prompt[:, 1])
        y_prompt = np.cumsum(prompt[:, 2])
        ax.plot(x_prompt, y_prompt, 'b-', linewidth=1)
        first_x = x_prompt[-1]
        first_y = y_prompt[-1]

    x = np.cumsum(stroke[:, 1]) + first_x
    y = np.cumsum(stroke[:, 2]) + first_y

    #print('entire stroke', x, y)
    size_x = x.max() - x.min() + 1.
    size_y = y.max() - y.min() + 1.

    f.set_size_inches(7., 4.)

    cuts = np.where(stroke[:, 0] == 1)[0]
    #print(f'np.where(stroke[:, 0] == 1): {np.where(stroke[:, 0] == 1)}')
    #print(f'plot_stroke: cuts={cuts}')
    start = 0

    for cut_value in cuts:
        #print('black', start, cut_value)
        #print(x[start:cut_value], y[start:cut_value])
        ax.plot(x[start:cut_value], y[start:cut_value],
                'k-', linewidth=3)
        # show pen up part in red
        if cut_value + 2 < len(y):
            #print('red', cut_value-1, cut_value+2)
            ax.plot(x[cut_value-1:cut_value+2], y[cut_value-1:cut_value+2],
                    'r-', linewidth=2)
        start = cut_value + 1

    # final stroke. Especially important if there were no penups specified
    last_cut = cuts[-1] if len(cuts) > 0 else 0
    ax.plot(x[last_cut:len(x)], y[last_cut:len(y)],
            'r-', linewidth=2)


    ax.axis('equal')
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)

    if remainder is not None:
        x_rem = np.cumsum(remainder[:, 1]) + first_x
        y_rem = np.cumsum(remainder[:, 2]) + first_y
        ax.plot(x_rem, y_rem, 'g-', linewidth=1)

    if save_name is None:
        pyplot.show()
    else:
      try:
        pyplot.savefig(
            save_name,
            bbox_inches='tight',
            pad_inches=0.5)
      except Exception:
        print("Error building image!: " + save_name)

    return f

#%%
from torch.utils.data import random_split
from torch.utils.data import DataLoader
import numpy as np

class SeqDataset(Dataset):
    def __init__(self, strokes, stats=None):
        """data is a numpy array containing different seq*3 arrays"""
        #print(f'SeqDataSet.__init__: data.shape: {data.shape}')
        if stats is None:
            stats = self.calc_stats(strokes)
        self._stats = stats
        self.strokes = [stroke.copy() for stroke in strokes]
        for stroke in self.strokes:
            stroke[:, 1:] = stroke[:, 1:] - self._stats['mean'] / self._stats['std']
        self.shuffle()

    def calc_stats(self, values):
        totals = None
        meanUps = np.concatenate([stroke[:, 0] for stroke in values]).mean()
        shapes = [stroke[:, 1:] for stroke in values]
        totals = np.concatenate(shapes)
        return {'mean': totals.mean(axis=0), 'std': totals.std(axis=0), 'meanUps':meanUps}

    def stats(self):
        return self._stats

    def shuffle(self):
        # TODO(neil): almost-sorting would be better
        # TODO(neil): Call shuffle at beginning of each batch
        self.strokes = sorted(self.strokes, key=lambda s: s.shape[0])

    def __len__(self):
        #print(f'SeqDataSet len(self.data)={len(self.data)} self.seq_length={self.seq_length}')
        return len(self.strokes)

    def __getitem__(self, idx):
        t = torch.as_tensor(self.strokes[idx])
        return t[:-1], t[1:]

#%%
if False:
    datadir = hwdir / 'data'
    strokes = np.load(datadir / 'strokes-py3.npy', allow_pickle=True)
    strokes[0].shape[0]

    train_strokes = strokes[:1000]

    train_ds = SeqDataset(train_strokes)

    
    print(train_ds[1][0].shape)
    #print(train_ds.stats())

    def collate_fn(data):
        data.sort(key=lambda x: len(x[0]), reverse=True)
        xs = [d[0] for d in data]
        ys = [d[1] for d in data]
       
         
        return (pad_sequence(xs, batch_first=True, padding_value=-1),
                pad_sequence(ys, batch_first=True, padding_value=-1))

    dl = DataLoader(train_ds, batch_size=2, shuffle=False, collate_fn=collate_fn)
    x, y = iter(dl).next()
    print(f'x.shape: {x.shape}, y.shape: {y.shape}')
elif (False):
    datadir = hwdir / 'data'
    strokes = np.load(datadir / 'strokes-py3.npy', allow_pickle=True)
    strokes[0].shape[0]
    from collections import defaultdict

    d = defaultdict(int)

    for stroke in strokes:
        d[len(stroke)] += 1
    print(d)



#%%

class HWModel(pl.LightningModule):
  def __init__(self, lr=args['lr'], bs = args['batch_size'], 
        num_components=args['num_components'],
        tbtt_length=args['tbtt_length']):
    super().__init__()
    self.hidden_size = args["rnn_hidden_size"]
    self.rnn = nn.GRU(input_size=3, hidden_size=self.hidden_size, batch_first=True)
    self.fc = nn.Linear(self.hidden_size, 1+5*num_components) 

    self.learning_rate = lr
    self.bceWithLogitsLoss = torch.nn.BCEWithLogitsLoss()
    self.bs = bs
    self.num_components = num_components
    self.tbtt_length = tbtt_length

    # Return the hidden tensor(s) to pass to RNN

  def get_new_hidden(self, batch_size):
    if False:
        # LSTM
        return (torch.zeros(1, batch_size, self.hidden_size, device=self.device),
               torch.zeros(1, batch_size, self.hidden_size,device=self.device))
    else:
        return torch.zeros(1, batch_size, self.hidden_size, device=self.device)

  def forward(self, x, hidden):
    VERBOSE=False
    if VERBOSE:
        print(f'Forward: input: {x}, hidden: {hidden}')

    (x, hidden) = self.rnn(x, hidden)

    if VERBOSE:
      print(f'Forward: size after rnn: {x.size()}')
      print(f'Forward: after rnn: {x} ')
    x = self.fc(x)
    if VERBOSE:
      print(f'Forward: size after fc: {x.size()}')
      print(f'Forward:  after fc: {x}')
    nc = self.num_components
    # Order of output is:
    # 0: penup/down
    # [1:1+num_components]: mean-x
    # [1+num_components:1+2*num_components]: mean-y
    # [1+2*num_componets: 1+3_num_components: std-x
    # [1+3_num_componets: 1+4_num_components: std-y
    # [1+4_num_componets: 1+5_num_components: weighting factor
    x_new = x.clone()
    x_new[:,:, 1+2*nc:1+4*nc] = torch.exp(x[:,:, 1+2*nc:1+4*nc])   # convert to non-negative std dev
    x_new[:,:, 1+4*nc:1+5*nc] = F.softmax(x[:,:, 1+4*nc:1+5*nc], dim=2)   # convert to non-negative std dev
    return x_new, hidden
    
  def xy_loss(self, yhat, y, mask, verbose=False):
    nc = self.num_components
    if verbose:
        print(f'yhat: {yhat.shape}')
        print(f'y: {y.shape}')
    mean = yhat[:,:,0*nc:2*nc]
    stddev = yhat[:,:,2*nc:4*nc]
    weights = yhat[:, :, 4*nc:5*nc] # shape batch X seq x nc
    y_repeated = torch.cat([y[:,:,:1]]*nc + [y[:,:,1:]]*nc, dim=2)
    if verbose:
        print(f'mean: {mean.shape}')
        print(f'stddev: {stddev.shape}')
        print(f'weights: {weights.shape}')
        print(f'y_repeated: {y_repeated.shape}')
    y_repeated = y.repeat(1, 1, nc)  # because we must subtract from nc means
    weights_repeated = weights.repeat(1, 1, 2) # because we must weight 2 coords
    if verbose:
        print(f'weights_repeated: {weights_repeated.shape}')
    unweighted_losses = ((mean - y_repeated)**2)/(2*stddev**2)  + torch.log(stddev) # shape batch x seq x nc
    if verbose:
        print(f'1: unweighted_losses: {unweighted_losses.shape}')
        print(f'1: mask: {mask.shape}')
    weighted_losses = unweighted_losses * weights_repeated * mask
    if verbose:
        print(f'weighted_losses: {weighted_losses.shape}')
    result = torch.mean(weighted_losses)
    #print(f'result: {result}')

    if False:
        # no weights
        mean = yhat[:,:,0*nc:0*nc+2]
        stddev = yhat[:,:,2*nc:2*nc+2]
        losses = ((mean - y)**2)/(2*stddev**2)  + torch.log(stddev) # shape batch x seq x nc
        #print(f'2: losses: {losses}')
        result = torch.mean(losses)
        #print(f'old way: {result}')
    #print(f'new way: {result}')

    return result

  def loss(self, y_hat, y, verbose=False):
    #print(f'y_hat.shape, y.shape: {y_hat.shape} {y.shape}')
    #print(f'y_hat: [{y_hat[:,:20:]}, {y[:,:20,:]}')

    # mask out values where the penup < 0 (padded values are -1)
    # mask will be of shape (bs, seq_len, 1)
    mask = torch.where(y[:,:,0] < 0, 0.0, 1.0)
    mask = mask.unsqueeze(dim=-1)

    if verbose:
        print(f'mask: mask={mask}')
    xy_loss = self.xy_loss(y_hat[:,:,1:], y[:,:,1:], mask, verbose=verbose) 
    bce_loss = self.bceWithLogitsLoss(y_hat[:,:,:1]*mask, y[:,:,:1]*mask)
    #print(f'mse_loss: {mse_loss}, bce_loss = {bce_loss}')
    return xy_loss + bce_loss

  def step_tbptt(self, sub_batch, hiddens):
    data, y = sub_batch
    #print(f"step_tbptt: data.shape {data.shape}")
    #print(f"step_tbptt: y.shape {y.shape}")
    y_hat, hiddens = self(data, hiddens)
    return self.loss(y_hat, y)

  def training_step(self, batch, batch_idx):
    data, y = batch
    #print(f'training_step (data={data.shape}, y={y.shape}, batch_idx={batch_idx}')
    batch_size, seq_len, _ = data.shape
    hiddens = self.get_new_hidden(batch_size)
    tbtt_len = self.tbtt_length
    # TODO(neil): Handle last partial batch
    num_sub_batches = seq_len // tbtt_len
    loss = 0
    for i in range(num_sub_batches):
        # TODO(neil) make batch sizes *about* tbtt_len, but not exactly
        sub_batch = (data[:, i*tbtt_len:(i+1)*tbtt_len, :], y[:, i*tbtt_len:(i+1)*tbtt_len, :])
        loss = loss + self.step_tbptt(sub_batch, hiddens)
        hiddens = hiddens.detach()
    self.log('loss_train', loss, prog_bar=True)
    # self.logger.experiment.add_scalars("losses", {"train_loss": losses["total"]})
    return loss

  def validation_step(self, batch, batch_idx):
    #print(f'validation_step (batch={batch}, batch_idx={batch_idx}')
    data, y = batch
    hidden = self.get_new_hidden(batch_size=data.shape[0])
    y_hat, hidden = model(data, hidden)
    loss = self.loss(y_hat, y)
    #self.log("loss_val", losses)
    #self.logger.experiment.add_scalars("losses", {"val_loss": losses["total"]})
    #self.log('train_loss', losses, prog_bar=True)
    #self.log("val_accuracy", 100. * c / t)
    if batch_idx == 0:
      prompt=self.train_ds[0][0][:30,:]
      remainder=self.train_ds[0][0][30:,:]
      sample = self.generate_unconditionally(prompt)
      #print(f'sample: {sample}')
      print(f'generated HW epoch: {self.current_epoch}')
      f = plot_stroke(sample, prompt, remainder)
      self.logger.experiment.add_figure('generated HW', f, self.current_epoch)
      #self.logger.experiment.add_text('sample as tensor', str(sample.tolist()), self.current_epoch)
    return loss

  def generate_unconditionally(self, prompt=None, output_length=args["generated_length"],
        verbose=False):
    hidden = self.get_new_hidden(batch_size=1)
    output = torch.zeros((output_length+1, 3), device=self.device)

    def sample_from_prediction(prediction):
        prediction = prediction.cpu()
        result = torch.zeros((3))
        result[0] = 1 if  torch.sigmoid(prediction[0]) > random.random() else 0
        if True:
            # generate random values with given means and standard devs  
            nc = self.num_components
            #print(f'sample_from_prediction: prediction: {prediction}')
            means = prediction[1+0*nc:1+2*nc]
            stddev = prediction[1+2*nc:1+4*nc]
            #print(f'1: means: {means}, stddev: {stddev}')
            unweighted_xys = torch.normal(means, stddev)
            #print(f'unweighted_xys: {unweighted_xys}')
            #print(f'sample_from_prediction: means: {means}')
            #print(f'sample_from_prediction: stddev: {stddev}')
            #print(f'sample_from_prediction: unweighted_xys: {unweighted_xys}')
            weights = prediction[1+4*nc:1+5*nc]
            #print(f'sample_from_prediction: weights: {weights}')
            result[1] = torch.sum(unweighted_xys[:nc]*weights)
            result[2] = torch.sum(unweighted_xys[nc:]*weights)
            #print(f'result[1:]: {result[1:]}')
            #print(f'sample_from_prediction: result: {result}')
        else:
            result[1:]  = torch.normal(prediction[1:3], prediction[3:5])
        return result

    if prompt is not None:
        prompt = prompt.type_as(hidden)
        with torch.no_grad():
            input = torch.unsqueeze(prompt, 0)
        if verbose:
            print(f"prompt: input to forward: {input}")
        predictions, hidden = self.forward(input, hidden)
        if verbose:
            print(f"predictions: {predictions}")
        output[0] = sample_from_prediction(predictions[0, -1, :])

    for idx in range(output_length):
      with torch.no_grad():
        input = torch.reshape(output[idx], (1, 1, 3))
        predictions, hidden = self.forward(torch.reshape(output[idx], (1, 1, 3)), hidden)

      # Only use the last prediction.
      output[idx+1, :] = sample_from_prediction(predictions[0, -1, :])


    if prompt is not None:
        #skip first (zero) element
        output = output[1:,:]
    # convert to probabilities
    #output[:,0] = torch.sigmoid(output[:,0])
    asNumpy = output.cpu().numpy()
    # denormalize:
    asNumpy[:,1:] = asNumpy[:,1:] * self.stats['std'] + self.stats['mean']

    # Sample whether penUp or penDown based on probability of penUp
   # asNumpy[:, 0] = np.where(asNumpy[:, 0] > np.random.rand(asNumpy.shape[0]), 1, 0)
    return asNumpy

  def configure_optimizers(self):
   # print('self.learning_rate', self.learning_rate)
    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
    #scheduler = torch.optim.lr_scheduler.OneCycleLR(
        #optimizer,
        #max_lr=self.learning_rate,
        #total_steps=self.trainer.estimated_stepping_batches
    #)
    return [optimizer], []

  def prepare_data(self):
      datadir = hwdir / 'data'
      strokes = np.load(datadir / 'strokes-py3.npy', allow_pickle=True)
      self.strokes = strokes
  
  @staticmethod
  def dataloader_collate_fn(data):
    data.sort(key=lambda x: len(x[0]), reverse=True)
    xs = [d[0] for d in data]
    ys = [d[1] for d in data]
    
    return (pad_sequence(xs, batch_first=True, padding_value=-1),
            pad_sequence(ys, batch_first=True, padding_value=-1))


  def train_dataloader(self):
      return DataLoader(self.train_ds, shuffle=False, batch_size=self.bs, collate_fn=self.dataloader_collate_fn)

  def val_dataloader(self):
      return DataLoader(self.val_ds, shuffle=False, batch_size=self.bs, collate_fn=self.dataloader_collate_fn)


  def setup(self, stage = None):
      train_split = int(len(self.strokes)*0.95)
      valid_split = len(self.strokes) - train_split
      #train_strokes, valid_strokes = random_split(self.strokes, [train_split, valid_split])

      train_strokes = self.strokes[:train_split]
      valid_strokes = self.strokes[train_split:]

      self.train_ds = SeqDataset(train_strokes)
      self.stats = self.train_ds.stats()
      self.val_ds = SeqDataset(valid_strokes, self.stats)
      print(f'HWDataModule: len(train_ds) = {len(self.train_ds)}, len(val_ds)={len(self.val_ds)}')


model = HWModel()
if False:
    yhat=torch.tensor([
        [
            [0, 1, 2, 1, 1, 1],
            [0, 3, 4, 1, 1, 1],
        ],
        [
            [0, 5, 6, 2, 1, 1],
            [0, 7, 8, 3, 1, 1],
        ],
        ])
    y=torch.tensor([
        [
            [1, 0, 0],
            [1, 0, 0],
        ],
        [
            [0, 0, 0],
            [1, 0, 0],
        ],
        ])
else:
    yhat = torch.rand(512, 60, 6)
    y = torch.rand(512, 60, 2)
    mask = torch.zeros(512, 60, 1)

model.loss(yhat, y)
#%%




model = HWModel()

from pytorch_lightning.callbacks import LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger

logger = TensorBoardLogger(save_dir="/data/neil/runs", name="hw_generation-lightning")

lr_monitor = LearningRateMonitor(logging_interval='step')

class MyPrintingCallback(Callback):
    def on_train_start(self, trainer, pl_module):
        dl = model.train_dataloader()
        asNumpy = dl.dataset[0][0].numpy()
        print(f"asNumpy: {asNumpy.shape}")
        asNumpy[:,1:] = asNumpy[:,1:] * model.stats['std'] + model.stats['mean']
        print(f'original training image')
        f = plot_stroke(asNumpy)
        logger.experiment.add_figure('original training image', f, 0)


trainer = pl.Trainer(
        max_epochs=args["epochs"],
        num_sanity_val_steps=0,
        accelerator='gpu',
        devices=[1],
        logger=logger,
        log_every_n_steps=1,
        callbacks=[MyPrintingCallback()],
        )

logger.log_hyperparams(args)

trainer.fit(model)    

trainer

        # %%
